{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e69fa0b-12a7-4e6f-bc99-91a9b0a6ec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前處理公司代碼: 2880\n",
      "目前進度: 1 / 1\n",
      "所有資料下載完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    filename='mops_scraper.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_session():\n",
    "    \"\"\"建立並返回一個設定好的Session物件。\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': (\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "            '(KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "        ),\n",
    "        'Referer': 'https://mops.twse.com.tw/mops/web/t51sb10'\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    return session\n",
    "\n",
    "def build_initial_url(co_id, search_keyword, year, month, begin_day, end_day):\n",
    "    \"\"\"根據搜尋參數建立初始查詢的URL。\"\"\"\n",
    "    base_url = \"https://mops.twse.com.tw/mops/web/ajax_t51sb10\"\n",
    "    params = {\n",
    "        'encodeURIComponent': '1',\n",
    "        'step': '1',\n",
    "        'firstin': 'true',\n",
    "        'Stp': '4',\n",
    "        'go': 'false',\n",
    "        'r1': '2',\n",
    "        'co_id': co_id,\n",
    "        'KIND': 'L',\n",
    "        'keyWord': '自結',\n",
    "        'Condition2': '1',\n",
    "        'year': year,\n",
    "        'month1': month,\n",
    "        'begin_day': begin_day,\n",
    "        'end_day': end_day\n",
    "    }\n",
    "    return f\"{base_url}?{urlencode(params)}\"\n",
    "\n",
    "def extract_post_parameters(soup):\n",
    "    \"\"\"從初始HTML中提取詳細資料的POST參數。\"\"\"\n",
    "    regex = re.compile(r'document\\.fm\\.(\\w+)\\.value=\"([^\"]+)\"')\n",
    "    post_buttons = soup.find_all('input', {'type': 'button', 'onclick': True})\n",
    "    extracted_params = []\n",
    "\n",
    "    for button in post_buttons:\n",
    "        matches = regex.findall(button.get('onclick', ''))\n",
    "        if matches:\n",
    "            params_dict = {param: value for param, value in matches}\n",
    "            required_keys = ['seq_no', 'spoke_time', 'spoke_date', 'i', 'co_id', 'TYPEK']\n",
    "            if all(key in params_dict for key in required_keys):\n",
    "                extracted_params.append(params_dict)\n",
    "\n",
    "    logger.info(f\"成功提取到 {len(extracted_params)} 筆POST參數\")\n",
    "    return extracted_params\n",
    "\n",
    "def create_company_dir(base_directory='各金控重訊', co_id=''):\n",
    "    \"\"\"建立公司代碼的資料夾，若已存在則不執行任何操作。\"\"\"\n",
    "    directory = os.path.join(base_directory, co_id)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    return directory\n",
    "\n",
    "def build_detail_url(params_dict):\n",
    "    \"\"\"根據提取的參數建立詳細資料的URL。\"\"\"\n",
    "    base_url = \"https://mops.twse.com.tw/mops/web/ajax_t05st01\"\n",
    "    query_params = {\n",
    "        'step': '2',\n",
    "        'colorchg': '1',\n",
    "        'off': '1',\n",
    "        'firstin': '1'\n",
    "    }\n",
    "    query_params.update(params_dict)\n",
    "    return f\"{base_url}?{urlencode(query_params)}\"\n",
    "\n",
    "def save_html(content, directory, stock_code, announce_date):\n",
    "    \"\"\"將HTML內容保存為公司資料夾中的文件。\"\"\"\n",
    "    filename = f\"{stock_code}_detail_{announce_date}.html\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "    logger.info(f\"成功儲存HTML內容到 {file_path}\")\n",
    "\n",
    "def fetch_detail_page(session, detail_url):\n",
    "    \"\"\"發送POST請求並返回詳細頁面的HTML內容。\"\"\"\n",
    "    try:\n",
    "        response = session.post(detail_url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"發送POST請求失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # 要查詢的公司代碼列表'2880', '2881', '2882', '2883', '2884', '2885',  '2886', '2887', '2888', '2889', '2890', '2891', '2892', '5880'\n",
    "    \n",
    "    co_ids = ['2880']\n",
    "\n",
    "    # 重大資訊查詢設定\n",
    "    search_keyword = \"自結\"  \n",
    "    search_year = '112'\n",
    "    search_month = '12'\n",
    "    search_begin_day = '05'\n",
    "    search_end_day = '12'\n",
    "\n",
    "    session = setup_session()\n",
    "\n",
    "    # 遍歷所有公司代碼\n",
    "    for co_id in co_ids:\n",
    "        logger.info(f\"開始處理公司代碼: {co_id}\")\n",
    "        print(f\"目前處理公司代碼: {co_id}\")\n",
    "\n",
    "        # 建立該公司的資料夾\n",
    "        company_dir = create_company_dir(co_id=co_id)\n",
    "\n",
    "        # 建立初始查詢的URL\n",
    "        initial_url = build_initial_url(\n",
    "            co_id, search_keyword, search_year, search_month, \n",
    "            search_begin_day, search_end_day\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_initial = session.get(initial_url)\n",
    "            response_initial.raise_for_status()\n",
    "            logger.info(f\"成功下載 {co_id} 的初始頁面\")\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"下載初始頁面失敗（{co_id}）: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup_initial = BeautifulSoup(response_initial.text, 'html.parser')\n",
    "        post_params_list = extract_post_parameters(soup_initial)\n",
    "\n",
    "        if not post_params_list:\n",
    "            logger.error(f\"{co_id} 未提取到任何有效的POST參數，跳過此公司\")\n",
    "            continue\n",
    "\n",
    "        # 遍歷該公司代碼的所有POST參數\n",
    "        for idx, params_dict in enumerate(post_params_list, start=1):\n",
    "            logger.info(f\"處理第 {idx} 筆資料: {params_dict['co_id']}\")\n",
    "            print(f\"目前進度: {idx} / {len(post_params_list)}\")\n",
    "\n",
    "            detail_url = build_detail_url(params_dict)\n",
    "            html_detail = fetch_detail_page(session, detail_url)\n",
    "\n",
    "            if not html_detail:\n",
    "                logger.warning(f\"無法下載詳細頁面，跳過: {params_dict['co_id']}\")\n",
    "                continue\n",
    "\n",
    "            announce_date = params_dict['spoke_date']\n",
    "            stock_code = params_dict['co_id']\n",
    "            save_html(html_detail, company_dir, stock_code, announce_date)\n",
    "\n",
    "            time.sleep(5)  # 避免伺服器封鎖\n",
    "\n",
    "    logger.info(\"所有資料下載完成\")\n",
    "    print(\"所有資料下載完成\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f3ed0e-bbef-4d50-9463-a76f2b663734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前處理公司代碼: 2880\n",
      "目前進度: 1 / 1\n",
      "所有資料下載完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def setup_session():\n",
    "    \"\"\"建立並返回一個設定好的Session物件。\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': (\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "            '(KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "        ),\n",
    "        'Referer': 'https://mops.twse.com.tw/mops/web/t51sb10'\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    return session\n",
    "\n",
    "def build_initial_url(co_id, search_keyword, year, month, begin_day, end_day):\n",
    "    \"\"\"根據搜尋參數建立初始查詢的URL。\"\"\"\n",
    "    base_url = \"https://mops.twse.com.tw/mops/web/ajax_t51sb10\"\n",
    "    params = {\n",
    "        'encodeURIComponent': '1',\n",
    "        'step': '1',\n",
    "        'firstin': 'true',\n",
    "        'Stp': '4',\n",
    "        'go': 'false',\n",
    "        'r1': '2',\n",
    "        'co_id': co_id,\n",
    "        'KIND': 'L',\n",
    "        'keyWord': '自結',\n",
    "        'Condition2': '1',\n",
    "        'year': year,\n",
    "        'month1': month,\n",
    "        'begin_day': begin_day,\n",
    "        'end_day': end_day\n",
    "    }\n",
    "    return f\"{base_url}?{urlencode(params)}\"\n",
    "\n",
    "def extract_post_parameters(soup):\n",
    "    \"\"\"從初始HTML中提取詳細資料的POST參數。\"\"\"\n",
    "    regex = re.compile(r'document\\.fm\\.(\\w+)\\.value=\"([^\"]+)\"')\n",
    "    post_buttons = soup.find_all('input', {'type': 'button', 'onclick': True})\n",
    "    extracted_params = []\n",
    "\n",
    "    for button in post_buttons:\n",
    "        matches = regex.findall(button.get('onclick', ''))\n",
    "        if matches:\n",
    "            params_dict = {param: value for param, value in matches}\n",
    "            required_keys = ['seq_no', 'spoke_time', 'spoke_date', 'i', 'co_id', 'TYPEK']\n",
    "            if all(key in params_dict for key in required_keys):\n",
    "                extracted_params.append(params_dict)\n",
    "\n",
    "    return extracted_params\n",
    "\n",
    "def create_company_dir(base_directory='各金控重訊', co_id=''):\n",
    "    \"\"\"建立公司代碼的資料夾，若已存在則不執行任何操作。\"\"\"\n",
    "    directory = os.path.join(base_directory, co_id)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    return directory\n",
    "\n",
    "def build_detail_url(params_dict):\n",
    "    \"\"\"根據提取的參數建立詳細資料的URL。\"\"\"\n",
    "    base_url = \"https://mops.twse.com.tw/mops/web/ajax_t05st01\"\n",
    "    query_params = {\n",
    "        'step': '2',\n",
    "        'colorchg': '1',\n",
    "        'off': '1',\n",
    "        'firstin': '1'\n",
    "    }\n",
    "    query_params.update(params_dict)\n",
    "    return f\"{base_url}?{urlencode(query_params)}\"\n",
    "\n",
    "def fetch_detail_page(session, detail_url):\n",
    "    \"\"\"發送POST請求並返回詳細頁面的HTML內容。\"\"\"\n",
    "    try:\n",
    "        response = session.post(detail_url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    co_ids = ['2880']\n",
    "\n",
    "    # 重大資訊查詢設定\n",
    "    search_keyword = \"自結\"  \n",
    "    search_year = '112'\n",
    "    search_month = '11'\n",
    "    search_begin_day = '05'\n",
    "    search_end_day = '12'\n",
    "\n",
    "    session = setup_session()\n",
    "\n",
    "    # 遍歷所有公司代碼\n",
    "    for co_id in co_ids:\n",
    "        print(f\"目前處理公司代碼: {co_id}\")\n",
    "\n",
    "        # 建立該公司的資料夾\n",
    "        company_dir = create_company_dir(co_id=co_id)\n",
    "\n",
    "        # 建立初始查詢的URL\n",
    "        initial_url = build_initial_url(\n",
    "            co_id, search_keyword, search_year, search_month, \n",
    "            search_begin_day, search_end_day\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_initial = session.get(initial_url)\n",
    "            response_initial.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            continue\n",
    "\n",
    "        soup_initial = BeautifulSoup(response_initial.text, 'html.parser')\n",
    "        post_params_list = extract_post_parameters(soup_initial)\n",
    "\n",
    "        if not post_params_list:\n",
    "            continue\n",
    "\n",
    "        # 遍歷該公司代碼的所有POST參數\n",
    "        for idx, params_dict in enumerate(post_params_list, start=1):\n",
    "            print(f\"目前進度: {idx} / {len(post_params_list)}\")\n",
    "\n",
    "            detail_url = build_detail_url(params_dict)\n",
    "            html_detail = fetch_detail_page(session, detail_url)\n",
    "\n",
    "            if not html_detail:\n",
    "                continue\n",
    "            \n",
    "            time.sleep(5)  # 避免伺服器封鎖\n",
    "\n",
    "    print(\"所有資料下載完成\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53401d6d-032b-491f-abf5-118787c7b1f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html_detail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhtml_detail\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(html \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(page))\n\u001b[1;32m      8\u001b[0m htm \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mHTML(html)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html_detail' is not defined"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "for page in html_detail:\n",
    "    print(html = file.read(page))\n",
    "\n",
    "htm = etree.HTML(html)\n",
    "data = htm.xpath(\"//pre/text()\")[0]\n",
    "\n",
    "# 用正則表達式提取表格數據\n",
    "pattern = re.compile(r'(\\S+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)')\n",
    "matches = pattern.findall(data)\n",
    "\n",
    "# 將匹配結果轉換為 DataFrame\n",
    "columns = ['公司', '自結稅前淨利(億元)', '自結稅後淨利(億元)', '累計稅前淨利(億元)', '累計稅後淨利(億元)', '每股稅前盈餘(元)', '每股稅後盈餘(元)']\n",
    "df = pd.DataFrame(matches, columns=columns)#取第一行，第三列.iloc[0, 3] \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b62c20a-2931-4900-be05-79d5e786c3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net Income</th>\n",
       "      <th>Diluted EPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>21618294000.0</td>\n",
       "      <td>1.564356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>17308343000.0</td>\n",
       "      <td>1.257426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>17206199000.0</td>\n",
       "      <td>1.247525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>8653353000.0</td>\n",
       "      <td>0.628002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Net Income Diluted EPS\n",
       "2023-12-31  21618294000.0    1.564356\n",
       "2022-12-31  17308343000.0    1.257426\n",
       "2021-12-31  17206199000.0    1.247525\n",
       "2020-12-31   8653353000.0    0.628002"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Downloading the ticker data\n",
    "ticker = yf.Ticker(\"2880.tw\")\n",
    "\n",
    "# Getting the quarterly balance sheet, financials, and cashflow\n",
    "qbs = ticker.financials\n",
    "selected_items = [\"Net Income\", \"Diluted EPS\"]\n",
    "df_income = qbs.loc[selected_items].T\n",
    "df_income\n",
    "#selected_items = [\"Total Revenue\", \"Gross Profit\", \"Net Income\", \"Diluted EPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc12c4-51be-4d4b-a521-8fc4ed95c797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
